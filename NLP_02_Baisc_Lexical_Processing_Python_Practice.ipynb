{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequencies and Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency\n",
    "Description\n",
    "You learnt how to extract and plot word frequencies from a list of words. In this exercise, you need to extract the third most frequent word of a book (the book is provided) and print it's frequency.\n",
    "Execution Time Limit\n",
    "20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ISBE\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.gutenberg.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from nltk import FreqDist\n",
    "\n",
    "# load the ebook\n",
    "url = \"https://www.gutenberg.org/files/16/16-0.txt\"\n",
    "peter_pan = requests.get(url,verify = False).text\n",
    "\n",
    "# break the book into different words using the split() method\n",
    "peter_pan_words = peter_pan.split()# write your code here\n",
    "#sample_words = sample_text.split()\n",
    "\n",
    "\n",
    "# build frequency distribution using NLTK's FreqDist() function\n",
    "word_frequency = FreqDist(peter_pan_words) # write your code here\n",
    "#word_freq = FreqDist(words)\n",
    "\n",
    "# extract the frequency of third most frequent word\n",
    "freq = word_frequency.most_common(3)[2][1]\n",
    "\n",
    "# print the third most frequent word - don't change the following code, it is used to evaluate the code\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "Description\n",
    "In this exercise, you'll remove stop words in a given corpus of text of a book. Then, you'll print the frequency of the most frequent word.\n",
    "Execution Time Limit\n",
    "20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ISBE\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.gutenberg.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load the ebook\n",
    "url = \"https://www.gutenberg.org/files/16/16-0.txt\"\n",
    "peter_pan = requests.get(url, verify = False).text\n",
    "\n",
    "# break the book into different words using the split() method\n",
    "peter_pan_words = peter_pan.split()\n",
    "\n",
    "# extract nltk stop word list\n",
    "stopwords_new = stopwords.words('english')\n",
    "\n",
    "# remove 'stopwords' from 'peter_pan_words'\n",
    "no_stops =[word for word in peter_pan_words if word not in stopwords_new]\n",
    " # write code here\n",
    "\n",
    "# create word frequency of no_stops\n",
    "word_frequency = FreqDist(no_stops) # write code here\n",
    "\n",
    "# extract the most frequent word and its frequency\n",
    "frequency = word_frequency.most_common(1)[0][1]\n",
    "\n",
    "# print the third most frequent word - don't change the following code, it is used to evaluate the code\n",
    "print(frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation 1\n",
    "Description\n",
    "Write a piece of code that breaks a given sentence into words and store them in a list. Then print the length of the list. Use the NLTK tokeniser to tokenise words.\n",
    "\n",
    "Sample input:\n",
    "\n",
    "\"I love pasta\"\n",
    "\n",
    "Expected output:\n",
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'pasta']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import sys\n",
    "sentence = 'I love pasta'\n",
    "\n",
    "# tokenise sentence into words\n",
    "words =   word_tokenize(sentence) # write your code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation 2\n",
    "Description\n",
    "Write a piece of code that breaks a given sentence into words and stores them in a list. Then remove the stop words from this list and then print the length of the list. Again, use the NLTK tokeniser to do this.\n",
    "\n",
    "Sample input: \n",
    "“Education is the most powerful weapon that you can use to change the world” \n",
    "\n",
    "Expected output: \n",
    "6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "\n",
    "text= 'Education is the most powerful weapon that you can use to change the world'\n",
    "sentence = text # write code here\n",
    "\n",
    "# change sentence to lowercase\n",
    "sentence = sentence.lower() # write code here\n",
    "\n",
    "# tokenise sentence into words\n",
    "words =  word_tokenize(sentence) # write code here\n",
    "\n",
    "# extract nltk stop word list\n",
    "stopwords_new = stopwords.words('english') # write code here\n",
    "\n",
    "# remove stop words\n",
    "no_stops = [word for word in words if word not in stopwords_new]# write code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(len(no_stops))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Tokenisation 3\n",
    "Description\n",
    "Write a Python code using the NLTK library that breaks a given piece of text containing multiple sentences into different sentences. Finally print the total number of sentences in the text.\n",
    "\n",
    "Sample input: \n",
    "Develop a passion for your learning. If you do, you’ll never cease to grow.\n",
    "\n",
    "Expected output:\n",
    "\n",
    "2\n",
    "\n",
    "Execution Time Limit\n",
    "30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import ast, sys\n",
    "#text = sys.stdin.read()\n",
    "\n",
    "# change sentence to lowercase\n",
    "text = \"Develop a passion for your learning. If you do, you'll never cease to grow.\" # write code here\n",
    "\n",
    "# tokenise sentence into words\n",
    "sentences = sent_tokenize(text)# write code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "Description\n",
    "Use NLTK’s regex tokeniser to extract all the mentions from a given tweet and then print the total number of mentions. A mention comprises of a ‘@’ symbol followed by a username containing either alphabets, numbers or underscores.\n",
    "\n",
    "Sample tweet:\n",
    "So excited to be a part of machine learning and artificial intelligence program made by @upgrad and @iiitb\n",
    "\n",
    "Expected output:\n",
    "2 (because there are two mentions - ‘@upgrad’ and ‘@iiitb’ )\n",
    "\n",
    "\n",
    "Execution Time Limit\n",
    "10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import ast, sys\n",
    "#text = sys.stdin.read()\n",
    "# change text to lowercase\n",
    "text ='So excited to be a part of machine learning and artificial intelligence program made by @upgrad and @iiitb'  # write code here\n",
    "\n",
    "# pattern to extract mentions\n",
    "pattern = r\"@[\\w]+\" # write regex pattern here\n",
    "\n",
    "# extract mentions by using regex tokeniser\n",
    "mentions = regexp_tokenize (text, pattern) # write code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(len(mentions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider these documents and create a bag-of-words model with the frequency approach on these documents. Please note that there is no need to remove the stop words in this case (just for this exercise). After you’re done creating the model, answer the questions that follow.\n",
    "\n",
    "Document 1: “there was one place on my ankle that was itching”\n",
    "Document 2: “but we did not scratch it”\n",
    "Document 3: “and then my ear began to itch”\n",
    "Document 4: “and next my back”\n",
    "\n",
    "Question 1/2\n",
    "Mandatory\n",
    "Bag-of-words model\n",
    "What is the size of the bag-of-words matrix?\n",
    "\n",
    "A.  4 rows and 15 columns\n",
    "B.  4 rows and 23 columns\n",
    "C.  4 rows and 22 columns\n",
    "D.  4 rows and 24 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization\n",
    "Description\n",
    "Write a function which takes an input word and stems it by chopping off its suffix. For the sake of simplicity, consider only words that have the following suffixes:\n",
    "'ing'\n",
    "'ed'\n",
    "\n",
    "Sample input: ‘playing’\n",
    "Expected output: ‘play’\n",
    "\n",
    "Tip: Use regular expressions!\n",
    "Execution Time Limit\n",
    "15 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast, sys\n",
    "#word = sys.stdin.read()\n",
    "word = 'playing'\n",
    "# create function to chop off the suffixes 'ing' and 'ed'\n",
    "def stemmer(word):\n",
    "    word = re.sub('(ing|ed)$', '', word)\n",
    "    # write your code here   \n",
    "    return word\n",
    "\n",
    "# stem word -- don't change the following code, it is used to evaluate your code\n",
    "print(stemmer(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming\n",
    "Description\n",
    "Use Porter stemmer to stem a given word\n",
    "\n",
    "Sample input:\n",
    "Gardening\n",
    "\n",
    "Expected output:\n",
    "Garden\n",
    "Execution Time Limit\n",
    "15 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garden\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import ast, sys\n",
    "#word = sys.stdin.read()\n",
    "word= 'gardening'\n",
    "# instantiate porter stemmer\n",
    "stemmer = PorterStemmer() # write code here\n",
    "\n",
    "# stem word\n",
    "stemmed = stemmer.stem(word)# write your code here\n",
    "\n",
    "# print stemmed word -- don't change the following code, it is used to evaluate your code\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming\n",
    "Description\n",
    "Stem a given word using Snowball stemmer.\n",
    "\n",
    "Sample input:\n",
    "coming\n",
    "\n",
    "Expected output:\n",
    "come\n",
    "Execution Time Limit\n",
    "15 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import ast, sys\n",
    "#word = sys.stdin.read()\n",
    "word= 'coming'\n",
    "# instantiate porter stemmer\n",
    "stemmer = SnowballStemmer(\"english\") # write code here\n",
    "\n",
    "# stem word\n",
    "stemmed =stemmer.stem(word) # write code here\n",
    "\n",
    "# print stemmed word -- don't change the following code, it is used to evaluate your code\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF model\n",
    "Description\n",
    "You are given a set of documents in the code below. Calculate the tf-idf matrix and output the score of the term 'belt' in document two.\n",
    "Execution Time Limit\n",
    "10 seconds\n",
    "\n",
    "INPUT = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## below are imcoplete codes \n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# consider the following set of documents\n",
    "documents = [\"The coach lumbered on again, with heavier wreaths of mist closing round it as it began the descent.\",\n",
    "             \"The guard soon replaced his blunderbuss in his arm-chest, and, having looked to the rest of its contents, and having looked to the supplementary pistols that he wore in his belt, looked to a smaller chest beneath his seat, in which there were a few smith's tools, a couple of torches, and a tinder-box.\",\n",
    "            \"For he was furnished with that completeness that if the coach-lamps had been blown and stormed out, which did occasionally happen, he had only to shut himself up inside, keep the flint and steel sparks well off the straw, and get a light with tolerable safety and ease (if he were lucky) in five minutes.\",\n",
    "            \"Jerry, left alone in the mist and darkness, dismounted meanwhile, not only to ease his spent horse, but to wipe the mud from his face, and shake the wet out of his hat-brim, which might be capable of holding about half a gallon.\",\n",
    "            \"After standing with the bridle over his heavily-splashed arm, until the wheels of the mail were no longer within hearing and the night was quite still again, he turned to walk down the hill.\"]\n",
    "\n",
    "\n",
    "# preprocess document\n",
    "def preprocess(document):\n",
    "    'changes document to lower case, removes stopwords and stems words'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # stem\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document\n",
    "\n",
    "# preprocess documents using the preprocess function and store the documents again in a list\n",
    "documents = # write code here\n",
    "\n",
    "# create tf-idf matrix\n",
    "## write code here ##\n",
    "\n",
    "# extract score\n",
    "score = -1  # replace -1 with the score of 'belt' in document two. You can manually write the value by looking at the tf_idf model\n",
    "\n",
    "# print the score -- don't change the following piece od code, it's used to evaluate your code\n",
    "print(round(score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18)\t0.34706676322953556\n",
      "  (0, 2)\t0.34706676322953556\n",
      "  (0, 59)\t0.34706676322953556\n",
      "  (0, 12)\t0.34706676322953556\n",
      "  (0, 51)\t0.28001127926354535\n",
      "  (0, 88)\t0.34706676322953556\n",
      "  (0, 32)\t0.34706676322953556\n",
      "  (0, 46)\t0.34706676322953556\n",
      "  (0, 13)\t0.28001127926354535\n",
      "  (1, 7)\t0.17500574860015003\n",
      "  (1, 76)\t0.17500574860015003\n",
      "  (1, 79)\t0.17500574860015003\n",
      "  (1, 16)\t0.17500574860015003\n",
      "  (1, 78)\t0.17500574860015003\n",
      "  (1, 65)\t0.17500574860015003\n",
      "  (1, 61)\t0.17500574860015003\n",
      "  (1, 4)\t0.17500574860015003\n",
      "  (1, 64)\t0.17500574860015003\n",
      "  (1, 3)\t0.17500574860015003\n",
      "  (1, 87)\t0.17500574860015003\n",
      "  (1, 55)\t0.17500574860015003\n",
      "  (1, 75)\t0.17500574860015003\n",
      "  (1, 15)\t0.17500574860015003\n",
      "  (1, 58)\t0.17500574860015003\n",
      "  (1, 44)\t0.5250172458004501\n",
      "  :\t:\n",
      "  (3, 68)\t0.21666637672403882\n",
      "  (3, 48)\t0.21666637672403882\n",
      "  (3, 19)\t0.21666637672403882\n",
      "  (3, 17)\t0.21666637672403882\n",
      "  (3, 0)\t0.21666637672403882\n",
      "  (3, 41)\t0.21666637672403882\n",
      "  (3, 38)\t0.21666637672403882\n",
      "  (3, 20)\t0.1748050684985107\n",
      "  (3, 51)\t0.1748050684985107\n",
      "  (4, 34)\t0.2527726716084208\n",
      "  (4, 81)\t0.2527726716084208\n",
      "  (4, 80)\t0.2527726716084208\n",
      "  (4, 72)\t0.2527726716084208\n",
      "  (4, 56)\t0.2527726716084208\n",
      "  (4, 53)\t0.2527726716084208\n",
      "  (4, 31)\t0.2527726716084208\n",
      "  (4, 86)\t0.2527726716084208\n",
      "  (4, 43)\t0.2527726716084208\n",
      "  (4, 47)\t0.2527726716084208\n",
      "  (4, 84)\t0.2527726716084208\n",
      "  (4, 69)\t0.2527726716084208\n",
      "  (4, 33)\t0.2527726716084208\n",
      "  (4, 8)\t0.2527726716084208\n",
      "  (4, 70)\t0.2527726716084208\n",
      "  (4, 1)\t0.20393539986751064\n",
      "       alon       arm     began      belt   beneath    blown  blunderbuss  \\\n",
      "0  0.000000  0.000000  0.347067  0.000000  0.000000  0.00000     0.000000   \n",
      "1  0.000000  0.141194  0.000000  0.175006  0.175006  0.00000     0.175006   \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.20716     0.000000   \n",
      "3  0.216666  0.000000  0.000000  0.000000  0.000000  0.00000     0.000000   \n",
      "4  0.000000  0.203935  0.000000  0.000000  0.000000  0.00000     0.000000   \n",
      "\n",
      "        box     bridl      brim  ...     torch      turn      walk     well  \\\n",
      "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.00000   \n",
      "1  0.175006  0.000000  0.000000  ...  0.175006  0.000000  0.000000  0.00000   \n",
      "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.20716   \n",
      "3  0.000000  0.000000  0.216666  ...  0.000000  0.000000  0.000000  0.00000   \n",
      "4  0.000000  0.252773  0.000000  ...  0.000000  0.252773  0.252773  0.00000   \n",
      "\n",
      "        wet     wheel      wipe    within      wore    wreath  \n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.347067  \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.175006  0.000000  \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3  0.216666  0.000000  0.216666  0.000000  0.000000  0.000000  \n",
      "4  0.000000  0.252773  0.000000  0.252773  0.000000  0.000000  \n",
      "\n",
      "[5 rows x 89 columns]\n",
      "[[0.         0.         0.34706676 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34706676 0.28001128 0.         0.         0.         0.\n",
      "  0.34706676 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.34706676 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.34706676 0.\n",
      "  0.         0.         0.         0.28001128 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.34706676\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.34706676]\n",
      " [0.         0.14119354 0.         0.17500575 0.17500575 0.\n",
      "  0.17500575 0.17500575 0.         0.         0.         0.3500115\n",
      "  0.         0.         0.         0.17500575 0.17500575 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.17500575 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.52501725 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.17500575 0.         0.17500575 0.17500575 0.\n",
      "  0.         0.17500575 0.         0.         0.17500575 0.17500575\n",
      "  0.17500575 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.17500575 0.17500575 0.\n",
      "  0.17500575 0.17500575 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.17500575 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.20715955\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16713502 0.20715955 0.         0.         0.\n",
      "  0.         0.         0.16713502 0.         0.20715955 0.20715955\n",
      "  0.20715955 0.         0.20715955 0.         0.         0.20715955\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.20715955 0.         0.20715955 0.20715955 0.\n",
      "  0.20715955 0.         0.         0.20715955 0.         0.\n",
      "  0.         0.         0.20715955 0.         0.         0.\n",
      "  0.20715955 0.         0.         0.         0.         0.\n",
      "  0.20715955 0.         0.         0.20715955 0.         0.\n",
      "  0.         0.20715955 0.         0.         0.         0.20715955\n",
      "  0.         0.20715955 0.20715955 0.         0.         0.20715955\n",
      "  0.         0.         0.         0.         0.20715955 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.21666638 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.21666638 0.21666638 0.\n",
      "  0.         0.         0.         0.         0.         0.21666638\n",
      "  0.         0.21666638 0.17480507 0.21666638 0.         0.\n",
      "  0.         0.21666638 0.         0.         0.21666638 0.\n",
      "  0.21666638 0.         0.         0.         0.         0.21666638\n",
      "  0.21666638 0.         0.21666638 0.         0.         0.21666638\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.21666638 0.21666638 0.         0.17480507 0.21666638 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.21666638 0.         0.         0.\n",
      "  0.         0.         0.21666638 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.21666638\n",
      "  0.         0.21666638 0.         0.         0.        ]\n",
      " [0.         0.2039354  0.         0.         0.         0.\n",
      "  0.         0.         0.25277267 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25277267 0.         0.25277267 0.25277267 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25277267 0.         0.         0.         0.25277267\n",
      "  0.         0.         0.         0.         0.         0.25277267\n",
      "  0.         0.         0.25277267 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25277267 0.25277267 0.\n",
      "  0.25277267 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.25277267 0.25277267 0.         0.\n",
      "  0.25277267 0.         0.25277267 0.         0.        ]]\n",
      "0.175\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# consider the following set of documents\n",
    "documents = [\"The coach lumbered on again, with heavier wreaths of mist closing round it as it began the descent.\",\n",
    "             \"The guard soon replaced his blunderbuss in his arm-chest, and, having looked to the rest of its contents, and having looked to the supplementary pistols that he wore in his belt, looked to a smaller chest beneath his seat, in which there were a few smith's tools, a couple of torches, and a tinder-box.\",\n",
    "            \"For he was furnished with that completeness that if the coach-lamps had been blown and stormed out, which did occasionally happen, he had only to shut himself up inside, keep the flint and steel sparks well off the straw, and get a light with tolerable safety and ease (if he were lucky) in five minutes.\",\n",
    "            \"Jerry, left alone in the mist and darkness, dismounted meanwhile, not only to ease his spent horse, but to wipe the mud from his face, and shake the wet out of his hat-brim, which might be capable of holding about half a gallon.\",\n",
    "            \"After standing with the bridle over his heavily-splashed arm, until the wheels of the mail were no longer within hearing and the night was quite still again, he turned to walk down the hill.\"]\n",
    "\n",
    "\n",
    "# preprocess document\n",
    "def preprocess(document):\n",
    "    'changes document to lower case, removes stopwords and stems words'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # stem\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document\n",
    "\n",
    "# preprocess documents using the preprocess function and store the documents again in a list\n",
    "documents =[preprocess(document) for document in documents ] # write code here\n",
    "\n",
    "# create tf-idf matrix\n",
    "# import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit and transform the documents to get the tf-idf matrix\n",
    "tf_idf_matrix = vectorizer.fit_transform(documents)\n",
    "print(tf_idf_matrix)\n",
    "\n",
    "tfidf = pd.DataFrame(tf_idf_matrix.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "print(tfidf)\n",
    "\n",
    "print(tf_idf_matrix.toarray())\n",
    "\n",
    "# extract score\n",
    "score = 0.175  # replace -1 with the score of 'belt' in document two. You can manually write the value by looking at the tf_idf model\n",
    "\n",
    "# print the score -- don't change the following piece od code, it's used to evaluate your code\n",
    "print(round(score, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bonus practice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happily  :  happili\n",
      "program  :  program\n",
      "programs  :  program\n",
      "programmer  :  programm\n",
      "programming  :  program\n",
      "programmers  :  programm\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "ps = PorterStemmer()\n",
    " \n",
    "# choose some words to be stemmed\n",
    "words = [\"happily\", \"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
    " \n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happily  :  happili\n",
      "program  :  program\n",
      "programs  :  program\n",
      "programmer  :  programm\n",
      "programming  :  program\n",
      "programmers  :  programm\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import ast, sys\n",
    "#word = sys.stdin.read()\n",
    "\n",
    "words = word.lower()\n",
    "# instantiate porter stemmer\n",
    "stemmer = SnowballStemmer(\"english\") # write code here\n",
    "\n",
    "# stem word\n",
    "stemmed =stemmer.stem(words) # write code here\n",
    "\n",
    "words = [\"happily\", \"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
    "# print stemmed word -- don't change the following code, it is used to evaluate your code\n",
    "for w in words:\n",
    "    print(w, \" : \", stemmer.stem(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
